# Final Discussion

## Phase 1: Data Processing

We have conducted two tests to evaluate the performance and speedup of these parallel implementations. First, we compare all four implementations: fully sequential, using multi-core processing only, multiple threading only, and multiple-core processing with each core utilizing many threads. This test was conducted with 50 stock tickers as a proof of concept. The speedup relative to the naive baseline is substantial such that with eight processors, we achieved a speedup of nearly 25x over the baseline. Evidently, we achieved approximately 6x speedup with only the application of multi-core processing. Given that our processing task must achieve data input and output, inherently sequential tasks, this speedup is consistent with Ahmdal's law of strong scaling. In a purely parallel program, we would expect 8x speedup, and thus the reduction can be attributed to the I/O constraints. Moreover, the incredible speedup achieved by implementation of a multithreaded download further emphasizes the I/O bound nature of our task. Even using only a single core achieves a speedup of nearly 5x.

Given the substantial speedup and built-in implementation of the multiple-threading download, we have demonstrated a larger scale test between the multiple-thread single core and multiple-thread, multiple core, using all 500 stock tickers, representing a more realistic view of our data processing task. The second test had shown a speedup of approximately 6.85x for the 8 core processing task. The scaling here closely follows Ahmdal's law, which is intuitive given that it is almost embarrassingly parallel. The small reduction in time is likely due to the I/O restrictions of creating the dataset. After processing, the sequences are concatenated into a single dataset to be fed to the model. For data-intensive tasks, this is almost always an issue and prevents full parallelization of the task. There are two further points to highlight. First, the multi-process mode with only a single process is slower (speedup of 0.96x) compared to the sequential version. This slight penalty shows the computational cost associated with orchestration of the `multiprocessing` library and likely the `fork` `join` model of waiting for the process to complete. Finally, the non-parallel version of this process takes approximately 36 minutes compared to slightly more than 5 minutes for the parallel version. Indeed, estimating this performance over the fully sequential version suggests the code would take approximately 2.5 hours! Thus, our solution here dramatically reduces the time needed to pull this data.

## Phase 2: Model Training

To evaluate the performance of the Ring Reduce algorithm, we test model training under two scenarios. First, on a single node with two GPUs. One of the biggest strengths of Horovod is its flexibility, as the API follows similarly along with OpenMPI. Thus, our second set of tests show the scalability of this approach across one to four nodes, each with a single GPU.

In both cases, we run two tests. The first shows the strong scaling of each approach and compares this to the theoretical performance. Next, we show the the time-per epoch and time-per-step by effective batch size. This is more of a weak scaling approach. By keeping the per-node batch size equal, we roughly fix the work done by node and can tackle more training because of this.

From the test results of the single node, multiple GPU per node scenario, we noted that even when scaling the batch size substantially, the accuracy remains quite high. These results are in line with Lanbouri and Achchab (2020). While none of our specifications achieve quite as accurate results, our model is trained on a stock-by-stock basis, and is thus in-sample for any individual stock, potentially more important than a market aggregate. From a scientific perspective, this validates the approach of utilizing multiple GPUs, which can substantially accelerate training.

## Phase 3: Real-Time Prediction

To evaluate the performance of the prediction speed, we varied the number of stocks to predict on a single AWS instance, and recorded the total time it took to predict the stock prices at each update event.

With this, we saw that with a fully trained model, we were able to perform predictions real-time at approximately 0.045s/stock or 22 stocks/s on a relatively light-weight hardware. This means that we would be able to bring real-time stock prediction close to the end-user with COTS machines to increase the access to our system. Separately, depending on the time-budget requirement of the end-user, we can scale the problem size (number of stocks to predict) to meet the time budget requirement.

We also noticed that the latest data pulled from `yfinance` may contain NaN values. Hence, it is important to provision for the additional processing to handle these NaN values, which would otherwise cause problem to the prediction using the LSTM model.

## General Discussion

Generally, we are satisfied the outcome of our project, and we were, across the board, able to improve the performance of each phase of the pipeline via various parallel programming models. 

Our development process was to first develop a sequential version of the pipeline, followed by a parallel version in each phase of the pipeline. Our key takeaways included the incredible speedup achieved by data processing phase's implementation of a multithreaded download greatly exemplified the I/O bound nature of our task, such that even using only a single core achieved a speedup of nearly 5x. We also appreciated the parallel implementation of the model training through the Horovod platform, which had validated the approach of utilizing multiple GPUs, which can substantially accelerate training. Finally, we saw the efficiency of using the Spark Programming Model for real-time processing of data, such that we could even use a relatively light-weight hardware to perform stock price prediction with a neural network.