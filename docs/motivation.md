## Problem Description & Motivation

One challenge of the twenty-first century computing paradigm is the acceleration and capture of increasing amounts of data, at more fine-grained resolutions. The financial industry provides one salient source of this phenomenon: as computing power and infrastructure scale, it is possible to not only make predictions for trading based days or weeks in the future, but rather minutes or seconds. In the world of high-frequency trading, low latency is paramount, enabling algorithmic traders to quickly arbitrage prices in order to beat bid-ask spreads [CITE]. While high frequency traders often rely on algorithms to receive signals from a market and act on it before third parties are able to, new methods for time-series prediction have also arisen, particularly in the case of recurrent neural networks. In this project, we seek to unite these two themes, developing a pipeline to (1) process scalable quantities of high-frequency financial transaction data, (2) utilize multiple nodes and GPUs to speed up training of sequential models which are notoriously difficult to parallelize, and (3) serve predictions based on the trained model in a distributed manner.

## Pipeline Overview

Our pipeline breaks down into the three phases described above. All testing done for these phases is described below, though could, in principle be combined into a single phase. However, each of these steps can be discretized. This is an important aspect when considering cost and the computational resources necessary to sustain our models, as users may wish to divide the resources necessary, either for time purposes or in order to reduce the financial constraints of scraping the data and running the models together.

1. **Data Processing**: [LINK!] The data processing pipeline retrieves the data from the `yfinance` API, a Python-based framework which accesses minute-by-minute trade data for all 500 stocks of the S&P500 from Yahoo Finance. Using multiple-thread and multiple-core processing, this pipeline is sped up significantly.
2. **Model Training**: [LINK!] To model this high-frequency sequential data, we create sequences of 60 minutes each, and feed these sequences into a version of a recurrent neural network known as a LSTM, or Long Short-Term Memory. These models, which exploit the sequential nature of the data, are difficult to parallelize. Thus, we implement two strategies: first, we use accelerated computing on a GPU to speed up matrix-multiplication, and second we orchestrate (a) multiple GPUs on a single node, and (b) up to four GPUs across multiple nodes. As discussed in the Model Training section, our code and framework are also scalable to the multi-GPU, multi-node case.
3. **Prediction**: Once the model has been trained, the next logical step is to serve predictions. We do so in a streamed manner. That is, each minute, we take new prices and volume for some number of stocks and serve predictions of the 5-minute ahead price, each minute.